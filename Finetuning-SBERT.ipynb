{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sentence-BERT](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "\n",
    "[Reference Code](https://www.pinecone.io/learn/series/nlp/train-sentence-transformers-softmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       "  'idx': Value(dtype='int32', id=None)},\n",
       " {'premise': Value(dtype='string', id=None),\n",
       "  'hypothesis': Value(dtype='string', id=None),\n",
       "  'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "snli = datasets.load_dataset(\"snli\")\n",
    "mnli = datasets.load_dataset(\"glue\", \"mnli\")\n",
    "mnli[\"train\"].features, snli[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of datasets to remove 'idx' column from\n",
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'idx' column from each dataset\n",
    "for column_names in mnli.column_names.keys():\n",
    "    mnli[column_names] = mnli[column_names].remove_columns(\"idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation_matched', 'validation_mismatched', 'test_matched', 'test_mismatched'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli.column_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([-1,  0,  1,  2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(mnli[\"train\"][\"label\"]), np.unique(snli[\"train\"][\"label\"])\n",
    "# snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "snli = snli.filter(lambda x: 0 if x[\"label\"] == -1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([0, 1, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(mnli[\"train\"][\"label\"]), np.unique(snli[\"train\"][\"label\"])\n",
    "# snli also have -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have your two DatasetDict objects named snli and mnli\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Merge the two DatasetDict objects\n",
    "raw_dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": datasets.concatenate_datasets([snli[\"train\"], mnli[\"train\"]]).shuffle().select(list(range(100000))),\n",
    "        \"test\": datasets.concatenate_datasets([snli[\"test\"], mnli[\"test_mismatched\"]]).shuffle().select(list(range(10000))),\n",
    "        \"validation\": datasets.concatenate_datasets([snli[\"validation\"], mnli[\"validation_mismatched\"]]).shuffle().select(list(range(10000))),\n",
    "    }\n",
    ")\n",
    "# remove .select(list(range(1000))) in order to use full dataset\n",
    "# Now, merged_dataset_dict contains the combined datasets from snli and mnli\n",
    "raw_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab813a1b38594e748595686d915fef1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9961578358e42a9b65cd6fd566c4091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a153feb7af4a84b01fc209341e24a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_seq_length = 128\n",
    "    padding = \"max_length\"\n",
    "    # Tokenize the premise\n",
    "    premise_result = tokenizer(examples[\"premise\"], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    # num_rows, max_seq_length\n",
    "    # Tokenize the hypothesis\n",
    "    hypothesis_result = tokenizer(examples[\"hypothesis\"], padding=padding, max_length=max_seq_length, truncation=True)\n",
    "    # num_rows, max_seq_length\n",
    "    # Extract labels\n",
    "    labels = examples[\"label\"]\n",
    "    # num_rows\n",
    "    return {\n",
    "        \"premise_input_ids\": premise_result[\"input_ids\"],\n",
    "        \"premise_attention_mask\": premise_result[\"attention_mask\"],\n",
    "        \"hypothesis_input_ids\": hypothesis_result[\"input_ids\"],\n",
    "        \"hypothesis_attention_mask\": hypothesis_result[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"premise\", \"hypothesis\", \"label\"])\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch[\"premise_input_ids\"].shape)\n",
    "    print(batch[\"premise_attention_mask\"].shape)\n",
    "    print(batch[\"hypothesis_input_ids\"].shape)\n",
    "    print(batch[\"hypothesis_attention_mask\"].shape)\n",
    "    print(batch[\"labels\"].shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start from a pretrained bert-base-uncased model\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.load_state_dict(torch.load(\"bert_only_weights.pth\", map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
    "    return pool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function\n",
    "\n",
    "## Classification Objective Function \n",
    "We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference  $\\lvert u - v \\rvert $ and multiply the result with the trainable weight  $ W_t ∈  \\mathbb{R}^{3n \\times k}  $:\n",
    "\n",
    "$ o = \\text{softmax}\\left(W^T \\cdot \\left(u, v, \\lvert u - v \\rvert\\right)\\right) $\n",
    "\n",
    "where $n$ is the dimension of the sentence embeddings and k the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure 1.\n",
    "\n",
    "## Regression Objective Function. \n",
    "The cosine similarity between the two sentence embeddings $u$ and $v$ is computed (Figure 2). We use means quared-error loss as the objective function.\n",
    "\n",
    "(Manhatten / Euclidean distance, semantically  similar sentences can be found.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurations(u, v):\n",
    "    # build the |u-v| tensor\n",
    "    uv = torch.sub(u, v)  # batch_size,hidden_dim\n",
    "    uv_abs = torch.abs(uv)  # batch_size,hidden_dim\n",
    "\n",
    "    # concatenate u, v, |u-v|\n",
    "    x = torch.cat([u, v, uv_abs], dim=-1)  # batch_size, 3*hidden_dim\n",
    "    return x\n",
    "\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    similarity = dot_product / (norm_u * norm_v)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_head = torch.nn.Linear(768 * 3, 3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "optimizer_classifier = torch.optim.Adam(classifier_head.parameters(), lr=2e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\silan\\Desktop\\A4\\.venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(raw_dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps - warmup_steps)\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()\n",
    "\n",
    "scheduler_classifier = get_linear_schedule_with_warmup(\n",
    "    optimizer_classifier, num_warmup_steps=warmup_steps, num_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler_classifier.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d686463e99ed4a16b8ac7a98600cea61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\silan\\Desktop\\A4\\.venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 1.069691\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e36b6d1070442eda2867d53dd12a544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 1.135245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288965f3fff84b1ab5aa0c9d2d7aab30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 1.097306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3970bbe5b57d4d81b78832d556e8945e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = 1.119025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c31784b8a5a464fbc9b16528785e506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | loss = 1.165591\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_epoch = 5\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    classifier_head.train()\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_classifier.zero_grad()\n",
    "\n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch[\"premise_input_ids\"].to(device)\n",
    "        inputs_ids_b = batch[\"hypothesis_input_ids\"].to(device)\n",
    "        attention_a = batch[\"premise_attention_mask\"].to(device)\n",
    "        attention_b = batch[\"hypothesis_attention_mask\"].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "\n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u = model(inputs_ids_a, attention_mask=attention_a)\n",
    "        v = model(inputs_ids_b, attention_mask=attention_b)\n",
    "\n",
    "        u_last_hidden_state = u.last_hidden_state  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v_last_hidden_state = v.last_hidden_state  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "        # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a)  # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b)  # batch_size, hidden_dim\n",
    "\n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)  # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv)  # batch_size,hidden_dim\n",
    "\n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1)  # batch_size, 3*hidden_dim\n",
    "\n",
    "        # process concatenated tensor through classifier_head\n",
    "        x = classifier_head(x)  # batch_size, classifer\n",
    "\n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = criterion(x, label)\n",
    "\n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        scheduler.step()  # update learning rate scheduler\n",
    "        scheduler_classifier.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1} | loss = {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.42      0.02      0.05      3486\n",
      "      neutral       0.33      0.75      0.46      3199\n",
      "contradiction       0.33      0.25      0.28      3315\n",
      "\n",
      "     accuracy                           0.33     10000\n",
      "    macro avg       0.36      0.34      0.26     10000\n",
      " weighted avg       0.36      0.33      0.26     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.eval()\n",
    "classifier_head.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # prepare batches and move all to the active device\n",
    "        inputs_ids_a = batch[\"premise_input_ids\"].to(device)\n",
    "        inputs_ids_b = batch[\"hypothesis_input_ids\"].to(device)\n",
    "        attention_a = batch[\"premise_attention_mask\"].to(device)\n",
    "        attention_b = batch[\"hypothesis_attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "        # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u, attention_a)  # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v, attention_b)  # batch_size, hidden_dim\n",
    "\n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u_mean_pool, v_mean_pool)  # batch_size,hidden_dim\n",
    "        uv_abs = torch.abs(uv)  # batch_size,hidden_dim\n",
    "\n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u_mean_pool, v_mean_pool, uv_abs], dim=-1)  # batch_size, 3*hidden_dim\n",
    "\n",
    "        # process concatenated tensor through classifier_head\n",
    "        logits = classifier_head(x)  # batch_size, classifer\n",
    "\n",
    "        # get predictions\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"entailment\", \"neutral\", \"contradiction\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8248\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs_a = tokenizer(sentence_a, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    inputs_b = tokenizer(sentence_b, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = inputs_a[\"input_ids\"]\n",
    "    attention_a = inputs_a[\"attention_mask\"]\n",
    "    inputs_ids_b = inputs_b[\"input_ids\"]\n",
    "    attention_b = inputs_b[\"attention_mask\"]\n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentence_a = \"Your contribution helped make it possible for us to provide our students with a quality education.\"\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence A: A man is playing guitar on stage.\n",
      "Sentence B: A person is performing music.\n",
      "Cosine Similarity: 0.7259\n",
      "\n",
      "Sentence A: She is cooking dinner in the kitchen.\n",
      "Sentence B: A woman is preparing a meal.\n",
      "Cosine Similarity: 0.6535\n",
      "\n",
      "Sentence A: The children are playing in the park.\n",
      "Sentence B: Kids are having fun outdoors.\n",
      "Cosine Similarity: 0.6873\n",
      "\n",
      "Sentence A: He is reading a book quietly.\n",
      "Sentence B: A man is enjoying a book.\n",
      "Cosine Similarity: 0.6883\n",
      "\n",
      "Sentence A: The sun is shining brightly.\n",
      "Sentence B: I am planning to go for a walk.\n",
      "Cosine Similarity: 0.4353\n",
      "\n",
      "Sentence A: She bought a new dress.\n",
      "Sentence B: The store had a big sale yesterday.\n",
      "Cosine Similarity: 0.5045\n",
      "\n",
      "Sentence A: The car is parked outside.\n",
      "Sentence B: It might rain later in the evening.\n",
      "Cosine Similarity: 0.4254\n",
      "\n",
      "Sentence A: They are watching a movie.\n",
      "Sentence B: The theater was crowded last night.\n",
      "Cosine Similarity: 0.3582\n",
      "\n",
      "Sentence A: The dog is barking loudly.\n",
      "Sentence B: The neighborhood is completely silent.\n",
      "Cosine Similarity: 0.6425\n",
      "\n",
      "Sentence A: He passed the exam easily.\n",
      "Sentence B: He failed all his tests this semester.\n",
      "Cosine Similarity: 0.6576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    # Entailment pairs\n",
    "    (\"A man is playing guitar on stage.\", \"A person is performing music.\"),\n",
    "    (\"She is cooking dinner in the kitchen.\", \"A woman is preparing a meal.\"),\n",
    "    (\"The children are playing in the park.\", \"Kids are having fun outdoors.\"),\n",
    "    (\"He is reading a book quietly.\", \"A man is enjoying a book.\"),\n",
    "    # Neutral pairs\n",
    "    (\"The sun is shining brightly.\", \"I am planning to go for a walk.\"),\n",
    "    (\"She bought a new dress.\", \"The store had a big sale yesterday.\"),\n",
    "    (\"The car is parked outside.\", \"It might rain later in the evening.\"),\n",
    "    (\"They are watching a movie.\", \"The theater was crowded last night.\"),\n",
    "    # Contradiction pairs\n",
    "    (\"The dog is barking loudly.\", \"The neighborhood is completely silent.\"),\n",
    "    (\"He passed the exam easily.\", \"He failed all his tests this semester.\"),\n",
    "]\n",
    "\n",
    "for sentence_a, sentence_b in sentences:\n",
    "    similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "    print(f\"Sentence A: {sentence_a}\")\n",
    "    print(f\"Sentence B: {sentence_b}\")\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment Examples:\n",
      "Cosine Similarity: 0.8104\n",
      "Cosine Similarity: 0.8615\n",
      "Cosine Similarity: 0.8124\n",
      "Cosine Similarity: 0.7727\n",
      "Cosine Similarity: 0.9073\n",
      "\n",
      "Neutral Examples:\n",
      "Cosine Similarity: 0.7053\n",
      "Cosine Similarity: 0.5814\n",
      "Cosine Similarity: 0.8302\n",
      "Cosine Similarity: 0.9373\n",
      "Cosine Similarity: 0.7901\n",
      "\n",
      "Contradiction Examples:\n",
      "Cosine Similarity: 0.8581\n",
      "Cosine Similarity: 0.3708\n",
      "Cosine Similarity: 0.4233\n",
      "Cosine Similarity: 0.5587\n",
      "Cosine Similarity: 0.5595\n"
     ]
    }
   ],
   "source": [
    "# Randomly pick 5 pairs of sentences from snli with different entailment relationships\n",
    "import random\n",
    "\n",
    "snli_entailment = snli[\"validation\"].filter(lambda x: x[\"label\"] == 0)\n",
    "snli_neutral = snli[\"validation\"].filter(lambda x: x[\"label\"] == 1)\n",
    "snli_contradiction = snli[\"validation\"].filter(lambda x: x[\"label\"] == 2)\n",
    "\n",
    "random_entailment = random.sample(list(snli_entailment), 5)\n",
    "random_neutral = random.sample(list(snli_neutral), 5)\n",
    "random_contradiction = random.sample(list(snli_contradiction), 5)\n",
    "\n",
    "print(\"Entailment Examples:\")\n",
    "for example in random_entailment:\n",
    "    sentence_a = example[\"premise\"]\n",
    "    sentence_b = example[\"hypothesis\"]\n",
    "    similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "    # print(f\"Sentence A: {sentence_a}\")\n",
    "    # print(f\"Sentence B: {sentence_b}\")\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\nNeutral Examples:\")\n",
    "for example in random_neutral:\n",
    "    sentence_a = example[\"premise\"]\n",
    "    sentence_b = example[\"hypothesis\"]\n",
    "    similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "    # print(f\"Sentence A: {sentence_a}\")\n",
    "    # print(f\"Sentence B: {sentence_b}\")\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\nContradiction Examples:\")\n",
    "for example in random_contradiction:\n",
    "    sentence_a = example[\"premise\"]\n",
    "    sentence_b = example[\"hypothesis\"]\n",
    "    similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "    # print(f\"Sentence A: {sentence_a}\")\n",
    "    # print(f\"Sentence B: {sentence_b}\")\n",
    "    print(f\"Cosine Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entailment: 0.11354333 1.0 0.7228040125105752\n",
      "Neutral: 0.09101771 0.9933052 0.6847346374664277\n",
      "Contradiction: 0.12401185 0.9995548 0.6427295705584254\n"
     ]
    }
   ],
   "source": [
    "entailment_similarities = [calculate_similarity(model, tokenizer, example[\"premise\"], example[\"hypothesis\"], device) for example in snli_entailment]\n",
    "neutral_similarities = [calculate_similarity(model, tokenizer, example[\"premise\"], example[\"hypothesis\"], device) for example in snli_neutral]\n",
    "contradiction_similarities = [\n",
    "    calculate_similarity(model, tokenizer, example[\"premise\"], example[\"hypothesis\"], device) for example in snli_contradiction\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Entailment:\", min(entailment_similarities), max(entailment_similarities), sum(entailment_similarities) / len(entailment_similarities))\n",
    "print(\"Neutral:\", min(neutral_similarities), max(neutral_similarities), sum(neutral_similarities) / len(neutral_similarities))\n",
    "print(\n",
    "    \"Contradiction:\",\n",
    "    min(contradiction_similarities),\n",
    "    max(contradiction_similarities),\n",
    "    sum(contradiction_similarities) / len(contradiction_similarities),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Saved Successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save Fine-Tuned Model\n",
    "torch.save(model.state_dict(), \"sbert_finetuned.pth\")\n",
    "torch.save(classifier_head.state_dict(), \"classifier_head.pth\")\n",
    "\n",
    "print(\"Fine-Tuned Model Saved Successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
